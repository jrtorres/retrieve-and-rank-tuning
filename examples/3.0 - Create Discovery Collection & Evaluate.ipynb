{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 - Create Discovery Collection (& Evaluate Performance)\n",
    "\n",
    "Simple example which will:\n",
    "\n",
    "1. Create a Discovery Cluster\n",
    "2. Create an environment & new document collection\n",
    "3. Upload documents into that collection (from the same file that we used earlier for Solr)\n",
    "4. Evaluate the performance of the base retrieval (similar to 2.0 - Evaluate RnR Performance)\n",
    "\n",
    "To learn more about the data used in the experiment, see here: https://github.ibm.com/rchakravarti/rnr-debugging-scripts/tree/master/resources/insurance_lib_v2\n",
    "\n",
    "*Note:* Ensure credentials have been updated in config/config.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the necessary scripts and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config from /stuff/workspace/rnr-debugging-scripts/config/config.ini\n",
      "Using data from /stuff/workspace/rnr-debugging-scripts/resources/insurance_lib_v2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from os import path, getcwd\n",
    "import json\n",
    "from tempfile import mkdtemp\n",
    "import glob\n",
    "\n",
    "sys.path.extend([path.abspath(path.join(getcwd(), path.pardir))])\n",
    "\n",
    "from rnr_debug_helpers.utils.discovery_wrappers import DiscoveryProxy\n",
    "from rnr_debug_helpers.utils.io_helpers import load_config, smart_file_open, \\\n",
    "    RankerRelevanceFileQueryStream, initialize_query_stream, insert_modifier_in_filename, PredictionReader\n",
    "from rnr_debug_helpers.create_cross_validation_splits import split_file_into_train_and_test\n",
    "from rnr_debug_helpers.compute_ranking_stats import compute_performance_stats\n",
    "\n",
    "config_file_path = path.abspath(path.join(getcwd(), path.pardir, 'config', 'config.ini'))\n",
    "print('Using config from {}'.format(config_file_path))\n",
    "\n",
    "config = load_config(config_file_path=config_file_path)\n",
    "\n",
    "insurance_lib_data_dir = path.abspath(path.join(getcwd(), path.pardir, 'resources', 'insurance_lib_v2'))\n",
    "print('Using data from {}'.format(insurance_lib_data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a New Document Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Either re-use an existing collection id by over riding the below, or leave as is to create one\n",
    "collection_id = '96b27926-7d48-47dc-a0fd-2f7333e3d6e4'\n",
    "\n",
    "discovery = DiscoveryProxy(config)\n",
    "\n",
    "collection_id = discovery.setup_collection(collection_id=collection_id,\n",
    "                                           config_file=path.join(insurance_lib_data_dir,\n",
    "                                                                 'discovery_config.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Upload documents\n",
    " \n",
    "We use the same InsuranceLibV2 corpus file that had been pre-processed and formatted into the Solr format for adding documents.  Discovery essentially expects the document in a json format consisting of {`field name 1`:`field value 1`...}. So we iterate over the docs in the Solr format XML file and upload the file iteratively. \n",
    " \n",
    "Since Discovery doesn't (yet) have bulk upload, we speed up the uploads via multi-processing to speed it up.  This seems to misbehave in a python notebook, so run the actual upload in a regular python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"collection_id\": \"96b27926-7d48-47dc-a0fd-2f7333e3d6e4\",\n",
      "    \"configuration_id\": \"889a08c9-cad9-4287-a87d-2f0380363bff\",\n",
      "    \"created\": \"2017-05-19T01:50:05.372Z\",\n",
      "    \"description\": \"TestCollection\",\n",
      "    \"document_counts\": {\n",
      "        \"available\": 27413,\n",
      "        \"failed\": 0,\n",
      "        \"processing\": 0\n",
      "    },\n",
      "    \"language\": \"en\",\n",
      "    \"name\": \"TestCollection-InsLibV2\",\n",
      "    \"status\": \"active\",\n",
      "    \"training_status\": {\n",
      "        \"available\": false,\n",
      "        \"data_updated\": \"\",\n",
      "        \"minimum_examples_added\": false,\n",
      "        \"minimum_queries_added\": false,\n",
      "        \"notices\": 0,\n",
      "        \"processing\": false,\n",
      "        \"successfully_trained\": \"\",\n",
      "        \"sufficient_label_diversity\": false,\n",
      "        \"total_examples\": 0\n",
      "    },\n",
      "    \"updated\": \"2017-05-19T01:50:05.372Z\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "def _parse_doc_elements(element):\n",
    "    \"\"\"\n",
    "    Parses a single document element from the Solr format xml element into a format Discovery can understand\n",
    "    :param element:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    doc_id, body = None, None\n",
    "    for field in element.findall(\"field\"):\n",
    "        if field.attrib['name'] == 'id':\n",
    "            doc_id = field.text\n",
    "        elif field.attrib['name'] == 'body':\n",
    "            body = field.text\n",
    "    if doc_id is None or body is None:\n",
    "        raise ValueError('Unable to parse id and body from xml entry: %s' % element)\n",
    "    return doc_id, {'body': body}\n",
    "\n",
    "\n",
    "def document_corpus_as_iterable(corpus):\n",
    "    stats = defaultdict(int)\n",
    "    with smart_file_open(corpus) as infile:\n",
    "        for event, element in ET.iterparse(infile):\n",
    "            if event == 'end' and element.tag == 'doc':\n",
    "                stats['num_xml_entries'] += 1\n",
    "                yield _parse_doc_elements(element)\n",
    "\n",
    "\n",
    "# This thing seems to misbehave when run from python notebooks due to its use of multiprocessing, so just run in a script\n",
    "#     discovery.upload_documents(collection_id=collection_id,\n",
    "#                                corpus=document_corpus_as_iterable(\n",
    "#                                    path.join(insurance_lib_data_dir, 'document_corpus.solr.xml')))\n",
    "\n",
    "discovery.print_collection_stats(collection_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate performance of base Discovery (natural_language_query)\n",
    "\n",
    "First we generate a train and test split so that we can compare with the performance of Discovery after we add training data.  \n",
    "\n",
    "**Note:** Since we do not have control over training the same way we did with RetrieveAndRank, it is difficult to use the cross-validation splits for evaluation as we did in [2.0 - Evaluate RnR Performance](https://github.ibm.com/rchakravarti/rnr-debugging-scripts/blob/master/examples/2.0%20-%20Evaluate%20RnR%20Performance.ipynb).  Instead, as soon as training data is uploaded to the service, it gets consumed by Discovery (with some latency).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-24 20:13:24,716 INFO create_cross_validation_splits.py - Creating train split with 1600 queries (80.0 %) and test split with 400 queries\n",
      "2017-05-24 20:13:24,718 WARNING create_cross_validation_splits.py - Path <</tmp/tmpf_8p0nih>> already exists, files may be overwritten\n",
      "2017-05-24 20:13:24,764 INFO create_cross_validation_splits.py - Wrote 1600 train instances and 400 test instances to <</tmp/tmpf_8p0nih>>\n",
      "\n",
      "Created train and validation splits in directory: /tmp/tmpf_8p0nih\n",
      "/tmp/tmpf_8p0nih/validation.relevance_file.csv\n",
      "/tmp/tmpf_8p0nih/train.relevance_file.csv\n"
     ]
    }
   ],
   "source": [
    "experimental_directory = mkdtemp()\n",
    "\n",
    "with smart_file_open(path.join(insurance_lib_data_dir, 'validation_gt_relevance_file.csv')) as infile:\n",
    "    split_file_into_train_and_test(initialize_query_stream(infile, file_format='relevance_file'),\n",
    "                                experimental_directory, train_percentage=0.80)\n",
    "\n",
    "print('\\nCreated train and validation splits in directory: {}'.format(experimental_directory))\n",
    "for filename in glob.glob('{}/*.csv'.format(experimental_directory), recursive=True):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate predictions for each fold and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-19 01:16:03,890 INFO DiscoveryProxy - Sending runtime requests from <<LabelledQueryStream(fh: <_io.TextIOWrapper name='/tmp/tmpgv6lypfw/validation.relevance_file.csv' mode='r' encoding='utf-8'>)>> to collection: <<96b27926-7d48-47dc-a0fd-2f7333e3d6e4>> (predictions will be written to: <</tmp/tmpgv6lypfw/validation.relevance_file.discovery_predictions.txt>>)\n",
      "2017-05-19 01:21:10,722 INFO DiscoveryProxy - Completed getting runtime predictions for 400 questions\n",
      "{\n",
      "    \"num_questions\": 400.0,\n",
      "    \"num_results_returned\": 40000.0\n",
      "}2017-05-19 01:21:10,727 INFO Computing Accuracy - first lookup ground truth, then iterate over prediction file\n",
      "2017-05-19 01:21:10,755 INFO Computing Accuracy - Done reading in ground truth\n",
      "{\n",
      "    \"num_instances\": 662,\n",
      "    \"num_possible_correct\": 662,\n",
      "    \"num_questions\": 400\n",
      "}\n",
      "Test Performance\n",
      "{\n",
      "    \"average_precision_50_truncated\": 0.17131198693595093,\n",
      "    \"ndcg@50\": 0.2561891147863539,\n",
      "    \"num_instances\": 40309,\n",
      "    \"num_queries_predicted\": 400,\n",
      "    \"num_queries_skipped_in_preds_file\": 0,\n",
      "    \"num_questions\": 400,\n",
      "    \"top-1-accuracy\": 0.125\n",
      "}"
     ]
    }
   ],
   "source": [
    "rows=100\n",
    "ndcg_evaluated_at = 50\n",
    "\n",
    "test_set = path.join(experimental_directory, 'validation.relevance_file.csv')\n",
    "prediction_file = insert_modifier_in_filename(test_set,'discovery_predictions','txt')\n",
    "    \n",
    "with smart_file_open(test_set) as infile:\n",
    "    # generate predictions\n",
    "    labelled_test_questions = RankerRelevanceFileQueryStream(infile)\n",
    "    json.dump(discovery.generate_natural_language_prediction_scores(\n",
    "        test_questions=labelled_test_questions, num_rows=rows,\n",
    "        prediction_file_location=prediction_file, collection_id=collection_id), sys.stdout, sort_keys=True, indent=4)\n",
    "\n",
    "    # score them\n",
    "    labelled_test_questions.reset()\n",
    "    with smart_file_open(prediction_file) as preds_file:\n",
    "        prediction_reader = PredictionReader(preds_file)\n",
    "        stats, _ = compute_performance_stats(prediction_reader=prediction_reader,\n",
    "                                                      ground_truth_query_stream=labelled_test_questions,\n",
    "                                                      k=ndcg_evaluated_at)\n",
    "print('\\nTest Performance')\n",
    "json.dump(stats, sys.stdout, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate performance of Discovery (natural_language_query) after uploading training data\n",
    "__WIP__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}